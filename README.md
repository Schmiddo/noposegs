# Look Gauss, No Pose! - Gaussian Splatting without accurate pose information

This repository contains code for the IROS24 paper "Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization" by Christian Schmidt, Jens Piekenbrinck, and Bastian Leibe.

## Installation

Clone the repository with submodules: `git clone https://github.com/Schmiddo/NoPoseGS.git --recursive`.
The renderer needs a working cuda compiler; it should work with all versions 11.8+.
Run the following commands to install the needed requirements:
```
pip install -r requirements.txt
pip install submodules/lietorch submodules/cudaops submodules/simple-knn submodules/diff-gaussian-rasterization --no-build-isolation
```
Compiling all extensions might take several minutes.
If you utilize multiple different GPUs, we recommend to add all needed compute capabilities to the `TORCH_CUDA_ARCH_LIST` environment variable before installing the extensions, e.g., add `export TORCH_CUDA_ARCH_LIST="6.1;7.5;8.6;8.9"` to support 1080, TitanRTX, 3090, and 4090 GPUs.

## Data preparation
For the experiments, we use three datasets: LLFF, Replica, and Tanks&Temples.
You will need to download the data and precompute monocular depth estimates for all frames.

### Data download

- The LLFF dataset was originally linked [in the LLFF repo](https://github.com/Fyusion/LLFF), but the link seems to be broken at the moment. Alternatively, you can download it from [kaggle](https://www.kaggle.com/datasets/arenagrenade/llff-dataset-full/data). Our dataloading scripts expects the down-scaled images to have the same names as the original ones; you can run `python rename.py <path-to-llff>` to create hardlinks with the correct name.
- For replica, we use data generated by iMap and hosted by [Nice-Slam](https://github.com/cvg/nice-slam); download [here](https://cvg-data.inf.ethz.ch/nice-slam/data/Replica.zip).
- For Tanks&Temples, we use the data provided by [NopeNerf](https://github.com/ActiveVisionLab/nope-nerf).

Extract all of these datasets to `data/` or point the environment variable `$DATA_DIR` to your data directory.

### Compute depth estimates
We use the same depth estimator as NopeNerf, from "Vision Transformers for Dense Prediction" ([DPT](https://github.com/isl-org/DPT)).
First download the weights from [here](https://drive.google.com/file/d/1dgcJEYYw1F8qirXhZxgNK8dWWz_8gZBD/view?usp=sharing) and put them under `models/` or point `$MODELDIR` to the directory containing the weights.
Then run `python -m scripts.generate_depth -llff -rep -tatnn` to precompute depth estimates.

## Training/Evaluation

We provide two experiment setups: camera pose estimation with respect to a trained model, and optimizing a scene without accurate camera poses.
To reproduce the first set of experiments, run `./run/noposegs_camopt.sh`.
This script creates models for all scenes of the llff dataset and then runs the pose estimation experiments.
To run the second set of experiments, run `./run/noposegs.sh`.
This script runs joint reconstruction and camera pose refinement on llff, replica, and tanks & temples.
We also provide run scripts for the baseline, vanilla 3DGS from COLMAP poses (optionally with noise), using `./run/3dgs.sh`.

## References

This repository is based on the original 3DGS implementation:

Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, George Drettakis: 3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)
